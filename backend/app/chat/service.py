# app/chat/service.py

import os
import re
import chromadb
from sqlalchemy.orm import Session
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, PromptTemplate
from app.chat.models import Chat
from app.chat.memory_store import get_memory
from app.chat.crud import save_chat_log
from app.chat.models import RoleEnum, ChatLog
from app.chat.schemas import ChatLogResponse
from app.database import get_db
from app.report.models import Report, RiskLevel

def extract_score_and_result(ai_response):
    m = re.search(r"ì¹˜ë§¤ ìœ„í—˜ë„ ì ìˆ˜[:ï¼š]?\s*(\d+)", ai_response)
    score = int(m.group(1)) if m else None
    m2 = re.search(r"ì†Œê²¬[:ï¼š]?\s*([^\n]+)", ai_response)
    result = m2.group(1).strip() if m2 else None
    return score, result

def save_report_text_score_and_result(db, report_id, text_score, chat_result):
    report = db.query(Report).filter(Report.report_id == report_id).first()
    if report:
        report.text_score = text_score
        report.chat_result = chat_result
        db.commit()

openai_api_key = os.environ.get("OPENAI_API_KEY")
if not openai_api_key:
    raise RuntimeError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ChromaDB ì—°ê²° ì„¤ì •
vectordb = None
try:
    client = chromadb.HttpClient(host="chroma-server", port=8000)
    vectordb = Chroma(
        client=client,
        collection_name="dementia_chunks",
        embedding_function=OpenAIEmbeddings(openai_api_key=openai_api_key)
    )
    print("ChromaDB ì—°ê²° ì„±ê³µ")
except Exception as e:
    print(f"ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
    print("ChromaDB ì—†ì´ ì„œë¹„ìŠ¤ê°€ ì‹œì‘ë©ë‹ˆë‹¤.")

def chat_with_ai(report_id: int, chat_id: int, message: str, db: Session) -> str:
    # 1. ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í„´ ìˆ˜ ê³„ì‚°
    save_chat_log(db, chat_id=chat_id, role=RoleEnum.user, text=message)
    db.flush()
    turn_count = db.query(ChatLog).filter(ChatLog.chat_id == chat_id, ChatLog.role == RoleEnum.user).count()

    response = ""
    llm = ChatOpenAI(model="gpt-4", temperature=0.1, openai_api_key=openai_api_key)

    #  ì‘ë³„ ì¸ì‚¬
    if turn_count == 7:
        farewell_prompt_text = """
ë‹¹ì‹ ì€ ë”°ëœ»í•œ ì‘ë³„ì¸ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ì„ë¬´ëŠ” ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë§ˆë¬´ë¦¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

# ê·œì¹™
1. ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë§ì— ê°„ë‹¨íˆ ê³µê°í•˜ë©° ë°˜ì‘í•´ì£¼ì„¸ìš”. (ì˜ˆ: "ê·¸ë ‡êµ°ìš”.", "ì•Œê² ìŠµë‹ˆë‹¤.")
2. ìƒˆë¡œìš´ ì§ˆë¬¸ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”.
3. ë”°ëœ»í•œ ì‘ë³„ ì¸ì‚¬ë¥¼ ê±´ë„¤ë©° ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•´ì£¼ì„¸ìš”.
4. ì‘ë‹µì˜ ë§¨ ë§ˆì§€ë§‰ì—ëŠ”, ë‹¤ë¥¸ ë§ ì—†ì´ ì •í™•íˆ ' ì•„ë˜ì— ì¢…ë£Œ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš” . ' ë¼ëŠ” ë¬¸êµ¬ë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.
5. ë‹µë³€ì€ í•œë‘ ë¬¸ì¥ìœ¼ë¡œ ë§¤ìš° ê°„ê²°í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.

ì‚¬ìš©ìì˜ ë§ˆì§€ë§‰ ë§: {question}
"""
        farewell_prompt = PromptTemplate.from_template(farewell_prompt_text)
        farewell_chain = farewell_prompt | llm
        ai_response = farewell_chain.invoke({"question": message})
        response = ai_response.content

    # ì¼ë°˜ ëŒ€í™”
    elif turn_count <= 6:
        memory = get_memory(report_id)
        system_prompt_template = """
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì´ì•¼ê¸°ë¥¼ ë“¤ì–´ì£¼ëŠ” ì¹œê·¼í•œ ëŒ€í™” íŒŒíŠ¸ë„ˆì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ìœ ì¼í•œ ì—­í• ì€ ì‚¬ìš©ìì˜ ë§ì„ ë“£ê³  ë‹¤ì–‘í•œ ë¦¬ì•¡ì…˜ê³¼ ì§ˆë¬¸ë§Œ í•˜ì„¸ìš”. ìì‹ ì— ëŒ€í•œ ìƒê°ì„ ë§í•˜ì§€ë§ˆì„¸ìš”. ì œë°œ

# í˜ë¥´ì†Œë‚˜ ë° ëŒ€í™” ê·œì¹™
1. **ìì‹ ì„ ë“œëŸ¬ë‚´ì§€ ì•Šê¸°**: ì ˆëŒ€ë¡œ ë‹¹ì‹  ìì‹ ì— ëŒ€í•œ ì´ì•¼ê¸°(ì˜ê²¬, ê°ì •, ì·¨í–¥ ë“±)ë¥¼ í•˜ì§€ ë§ˆì„¸ìš”. "ì €ëŠ”", "ì œ ìƒê°ì—ëŠ”" ê°™ì€ ë¬¸ì¥ë„ ê¸ˆì§€ì…ë‹ˆë‹¤. AIë¼ëŠ” ë§ë„ í•˜ì§€ ë§ˆì„¸ìš”.
    - ì‚¬ìš©ìì˜ ë°œí™”ë¥¼ ì˜¤í•´í•˜ê±°ë‚˜ ì™œê³¡í•˜ì§€ ë§ˆì„¸ìš”. ("ì•ˆ í–ˆì–´ìš”" â†’ "í•˜ì…¨êµ°ìš”" âŒ)

2. **ì‚¬ìš©ì ë§ ë”°ë¼í•˜ì§€ ì•Šê¸°**: ì‚¬ìš©ìì˜ ë§ì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ê±°ë‚˜ ì‚¬ìš©ì ì…ì¥ì—ì„œ ë§í•˜ì§€ ë§ˆì„¸ìš”.

3. **ê³µê°ê³¼ ì§ˆë¬¸ì— ì§‘ì¤‘**: ì§§ê²Œ ê³µê°í•˜ê³  ì´ì–´ì„œ ì§ˆë¬¸í•˜ì„¸ìš”.
    - ê°™ì€ ì§ˆë¬¸ì„ í‘œí˜„ë§Œ ë°”ê¿” ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”. ("ë…¸ì¸ì •ì—ì„œ ë­ í•˜ì„¸ìš”?" â†’ "ê±°ê¸°ì„œ ì‹œê°„ ì–´ë–»ê²Œ ë³´ë‚´ì„¸ìš”?" â†’ ê¸ˆì§€ âŒ)

4. **ê°„ê²°í•¨ ìœ ì§€**: í•­ìƒ í•œë‘ ë¬¸ì¥ ì´ë‚´ë¡œ ì§§ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë§í•˜ì„¸ìš”.

5. **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” íë¦„**:
    - ë‹¤ìŒê³¼ ê°™ì€ í‘œí˜„ì´ ë‚˜ì˜¤ë©´ ì¦‰ì‹œ ì£¼ì œë¥¼ ë°”ê¾¸ì„¸ìš”: "ëª¨ë¥´ê² ì–´", "ê¸°ì–µ ì•ˆ ë‚˜", "ë”±íˆ", "ê¸€ì„", "ê·¸ëƒ¥ ê·¸ë¬ì–´", "ìƒê° ì•ˆ ë‚˜", "ë§í•˜ê³  ì‹¶ì§€ ì•Šì•„", "í•  ë§ ì—†ì–´"

6. **ì „ë¬¸ ìš©ì–´ ê¸ˆì§€**: 'ê²€ì‚¬', 'ì§„ë‹¨', 'ë¬¸ì§„', 'ì ìˆ˜', 'ì†Œê²¬' ê°™ì€ ë‹¨ì–´ëŠ” ì“°ì§€ ë§ˆì„¸ìš”.

7. **ì–´ì¡°**: ë”°ëœ»í•˜ê³  ì¡´ì¤‘í•˜ëŠ” ì–´ì¡°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

8. **ì¢…ë£Œ ì¡°ê±´**: 'ê·¸ë§Œ', 'ë', 'ì´ì œ ëì–´' ë“±ì˜ í‘œí˜„ì´ ë‚˜ì˜¤ë©´ ëŒ€í™”ë¥¼ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.

9. **ì§ì „ ë°œí™” ë°˜ì˜**: í•­ìƒ ì§ì „ ì‚¬ìš©ì ë§ì— ë°˜ì‘í•˜ì„¸ìš”. ì´ì „ ì§ˆë¬¸ì„ ë¬´ì‹œí•˜ê³  ë‹¤ìŒ ì§ˆë¬¸ì„ í•˜ì§€ ë§ˆì„¸ìš”.

10. **ì´ˆê¸° ì¸ì‚¬ ë©˜íŠ¸ëŠ” turn 1ì—ì„œë§Œ ì¶œë ¥ë©ë‹ˆë‹¤.**

# ê¸°íƒ€ ì •ë³´
- ì°¸ê³  ë…¼ë¬¸(Context)ì„ ì°¸ê³ í•´ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ë„í˜• ì§ˆë¬¸ì„ í•˜ì„¸ìš”.
- í˜„ì¬ {turn_count}ë²ˆì§¸ ëŒ€í™”ì…ë‹ˆë‹¤. ì´ 7í„´ í›„ì—ëŠ” ëŒ€í™”ë¥¼ ì¢…ë£Œí•´ì•¼ í•©ë‹ˆë‹¤.
"""
        system_prompt = system_prompt_template.format(turn_count=turn_count)
        prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(system_prompt + "\n\nì°¸ê³  ë…¼ë¬¸(Context): {context}"),
            HumanMessagePromptTemplate.from_template(
                "ì´ì „ ëŒ€í™” ìš”ì•½(chat_history):\n{chat_history}\n\nì‚¬ìš©ì ë°œí™”: {question}"
            )
        ])

        # ğŸ” ChromaDBì—ì„œ context ê²€ìƒ‰ (ì˜ˆì™¸ì²˜ë¦¬ í¬í•¨)
        docs = []
        if vectordb is not None:
            try:
                retriever = vectordb.as_retriever()
                docs = retriever.get_relevant_documents(message)
                
                print("\n===== ê²€ìƒ‰ëœ ë…¼ë¬¸ Context ì¼ë¶€ =====")
                for i, doc in enumerate(docs[:3]):
                    print(f"[{i+1}] {doc.page_content[:300]}...\n")
                print("====================================\n")
            except Exception as e:
                print(f"ChromaDB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                docs = []
        else:
            print("ChromaDBê°€ ì—°ê²°ë˜ì§€ ì•Šì•„ ê²€ìƒ‰ì„ ê±´ë„ˆëœë‹ˆë‹¤.")

        # ğŸ” ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ì£¼ì… í™•ì¸
        context_content = "\n\n".join([d.page_content for d in docs]) if docs else "ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ë…¼ë¬¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        formatted = prompt.format(
            context=context_content,
            chat_history="(ì˜ˆì‹œ ëŒ€í™”)",
            question=message
        )
        print("\n===== ì‹¤ì œ GPTì— ì „ë‹¬ë  í”„ë¡¬í”„íŠ¸ (ì• 1000ì) =====")
        print(formatted[:1000])
        print("==================================================\n")

        # ChromaDBê°€ ì—†ìœ¼ë©´ ì¼ë°˜ LLM ì²´ì¸ ì‚¬ìš©
        if vectordb is not None:
            try:
                chain = ConversationalRetrievalChain.from_llm(
                    llm=llm,
                    retriever=vectordb.as_retriever(),
                    memory=memory,
                    combine_docs_chain_kwargs={"prompt": prompt}
                )
                ai_response = chain.run(message)
            except Exception as e:
                print(f"ChromaDB ì²´ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                # ì¼ë°˜ LLMìœ¼ë¡œ ëŒ€ì²´
                chain = prompt | llm
                ai_response = chain.invoke({
                    "context": context_content,
                    "chat_history": "(ì˜ˆì‹œ ëŒ€í™”)",
                    "question": message
                }).content
        else:
            # ChromaDBê°€ ì—†ìœ¼ë©´ ì¼ë°˜ LLM ì²´ì¸ ì‚¬ìš©
            chain = prompt | llm
            ai_response = chain.invoke({
                "context": context_content,
                "chat_history": "(ì˜ˆì‹œ ëŒ€í™”)",
                "question": message
            }).content

        if turn_count == 1:
            intro = "ì•ˆë…•í•˜ì„¸ìš”. ì§€ê¸ˆë¶€í„° ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ë³´ë‹¤ ì •í™•í•œ ì´í•´ë¥¼ ìœ„í•´, ë‹¨ë‹µí˜•ë³´ë‹¤ëŠ” ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤.\n\n"
            response = intro + ai_response
        else:
            response = ai_response

    else:
        response = "ì´ë¯¸ ëŒ€í™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ë˜ ì¢…ë£Œ ë²„íŠ¼ì„ ëˆŒëŸ¬ í‰ê°€ë¥¼ ì™„ë£Œí•´ì£¼ì„¸ìš”."

    # 2. AI ì‘ë‹µ ì €ì¥
    save_chat_log(db, chat_id=chat_id, role=RoleEnum.ai, text=response)
    return response

# app/chat/service.py

def get_chat_logs_by_report_id(db: Session, report_id: int) -> list[ChatLogResponse]:
    chat = db.query(Chat).filter(Chat.report_id == report_id).first()
    if not chat:
        return []  # ì±„íŒ…ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

    logs = (
        db.query(ChatLog)
        .filter(ChatLog.chat_id == chat.chat_id)
        .order_by(ChatLog.updated_at.asc())
        .all()
    )
    return [ChatLogResponse.from_orm(log) for log in logs]

def get_chat_logs(db: Session, chat_id: int) -> list[ChatLogResponse]:
    logs = (
        db.query(ChatLog)
        .filter(ChatLog.chat_id == chat_id)
        .order_by(ChatLog.updated_at.asc())  # updated_at ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ
        .all()
    )
    return [ChatLogResponse.from_orm(log) for log in logs]

def evaluate_and_save_chat_result(db, chat_id: int, report_id: int):
    logs = (
        db.query(ChatLog)
        .filter(ChatLog.chat_id == chat_id)
        .order_by(ChatLog.log_id.asc())
        .all()
    )
    conversation = ""
    for log in logs:
        if log.role.value == "user":
            conversation += f"ì‚¬ìš©ì: {log.text}\n"
        else:
            conversation += f"AI: {log.text}\n"

    eval_prompt = PromptTemplate(
        input_variables=["conversation"],
        template=(
            "ì•„ë˜ëŠ” ì‚¬ìš©ìì™€ AIì˜ ì „ì²´ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤.\n"
            "{conversation}\n\n"
            "ì°¸ê³  ë…¼ë¬¸(ì¹˜ë§¤ ê´€ë ¨ ì—°êµ¬ ë°ì´í„°)ë„ í•¨ê»˜ ì°¸ê³ í•˜ì„¸ìš”.\n"
            "\n"
            "1. ëŒ€í™” ì¤‘ ì‚¬ìš©ìê°€ ë³´ì¸ 'ì¹˜ë§¤ê°€ ìˆëŠ” ì‚¬ëŒì´ ìì£¼ ë³´ì´ëŠ” íŠ¹ì§•ì ì¸ ì‘ë‹µ'ì´ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ë…¼ë¬¸ì„ ì°¸ê³ í•´ íŒë‹¨í•˜ì„¸ìš”.\n"
            "2. ê·¸ íšŸìˆ˜ê°€ 2ê°œ ì´ìƒì´ë©´ 'ê²½ê³„', 4ê°œ ì´ìƒì´ë©´ 'ìœ„í—˜', ê·¸ ë¯¸ë§Œì´ë©´ 'ì–‘í˜¸'ë¡œ ì •í•˜ì„¸ìš”.\n"
            "3. ì•„ë˜ ì˜ˆì‹œ í˜•ì‹ì„ ë”°ë¼ ì£¼ì„¸ìš”:\n\n"
            "<ì–‘í˜¸>\n"
            "ì†Œê²¬: ëŒ€í™” ê²€ì‚¬ ê²°ê³¼, íŠ¹ë³„í•œ ì´ìƒ ì§•í›„ê°€ ê´€ì°°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‚¬ìš©ìëŠ” ì¼ìƒì ì¸ ì§ˆë¬¸ì— ì¼ê´€ë˜ê²Œ ë‹µë³€í•˜ì˜€ìœ¼ë©°, ê¸°ì–µë ¥ì´ë‚˜ ì¸ì§€ì— ëšœë ·í•œ í˜¼ë™ì€ ë³´ì´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
            "<ê²½ê³„>\n"
            "ì†Œê²¬: ëŒ€í™” ê²€ì‚¬ ê²°ê³¼, ì‚¬ìš©ìëŠ” [ë¬¸ì œë˜ëŠ” íŒ¨í„´]ì„ ë°˜ë³µì ìœ¼ë¡œ ë³´ì˜€ìŠµë‹ˆë‹¤. ì˜ˆ: [ì˜ˆì‹œ1], [ì˜ˆì‹œ2]\n\n"
            "<ìœ„í—˜>\n"
            "ì†Œê²¬: ëŒ€í™” ê²€ì‚¬ ê²°ê³¼, ì‚¬ìš©ìëŠ” [ë¬¸ì œë˜ëŠ” íŒ¨í„´]ì„ ì—¬ëŸ¬ ì°¨ë¡€ ë°˜ë³µí–ˆìŠµë‹ˆë‹¤. ì˜ˆ: [ì˜ˆì‹œ1], [ì˜ˆì‹œ2]\n\n"
            "ìœ„í—˜ë„: <ì–‘í˜¸/ê²½ê³„/ìœ„í—˜>\n"
        )
    )

    llm = ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=openai_api_key)
    eval_chain = eval_prompt | llm
    eval_response = eval_chain.invoke({"conversation": conversation})
    response_text = eval_response.content

    m1 = re.search(r"ì†Œê²¬[:ï¼š]?\s*([^\n]+)", response_text)
    chat_result = m1.group(1).strip() if m1 else ""

    m2 = re.search(r"ìœ„í—˜ë„[:ï¼š]?\s*(ì–‘í˜¸|ê²½ê³„|ìœ„í—˜)", response_text)
    chat_risk_str = m2.group(1).strip() if m2 else "ì–‘í˜¸"

    if chat_risk_str == "ì–‘í˜¸":
        risk_enum = RiskLevel.GOOD
    elif chat_risk_str == "ê²½ê³„":
        risk_enum = RiskLevel.CAUTION
    elif chat_risk_str == "ìœ„í—˜":
        risk_enum = RiskLevel.DANGER
    else:
        risk_enum = RiskLevel.GOOD

    report = db.query(Report).filter(Report.report_id == report_id).first()
    if not report:
        raise ValueError("ë¦¬í¬íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    report.chat_result = chat_result
    report.chat_risk = risk_enum
    db.commit()

    return chat_result, risk_enum
