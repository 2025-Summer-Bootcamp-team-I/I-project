#app/chat/api.py
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File, Form
from sqlalchemy.orm import Session
from sse_starlette.sse import EventSourceResponse
import asyncio
import json
from celery.result import AsyncResult

from app.database import get_db
from app.auth.utils import get_current_user
from app.auth.models import User
from app.chat.schemas import (
    ChatRequest, ChatResponse, ChatLogResponse,
    CreateChatRequest, CreateChatResponse, EvaluateChatResponse, VoiceChatResponse
)
from app.chat.models import RoleEnum, Chat
from app.chat.service import (
    chat_with_ai, evaluate_and_save_chat_result,
    get_chat_logs_by_report_id, get_chat_logs
)
from app.chat.crud import save_chat_log
from app.chat.stream_handler import get_streaming_chain
from app.report.models import Report
from app.celery import celery_app


router = APIRouter(tags=["Chat"])


def is_end_message(message: str) -> bool:
    end_keywords = ["ë", "ê·¸ë§Œ", "ì¢…ë£Œ", "ë§ˆì¹˜ì", "ëë‚¼ë˜", "ëŒ€í™” ê·¸ë§Œ", "ëŒ€í™” ì¢…ë£Œ"]
    return any(kw in message for kw in end_keywords)

def is_gpt_end_response(response: str) -> bool:
    end_phrases = ["ì˜¤ëŠ˜ ëŒ€í™”ëŠ” ì—¬ê¸°ê¹Œì§€ í• ê²Œìš”. ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”."]
    return any(phrase in response for phrase in end_phrases)

#
@router.post(
    "",
    response_model=ChatResponse,
    summary="ì±„íŒ… ë©”ì‹œì§€ ì „ì†¡",
    description="ì±„íŒ… ë©”ì‹œì§€ë¥¼ ì „ì†¡í•˜ê³  AI ì‘ë‹µì„ ë°˜í™˜í•©ë‹ˆë‹¤."
)
def chat(
        request: ChatRequest,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    response = chat_with_ai(
        report_id=request.report_id,
        chat_id=request.chat_id,
        message=request.message,
        db=db
    )
    return {"response": response}

@router.post("/stream")
async def stream_chat(
        request: ChatRequest,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    import logging
    print(f"ğŸ”¥ STREAM API CALLED! Chat ID: {request.chat_id}, Message: {request.message}")
    logging.info(f"ğŸ”¥ STREAM API CALLED! Chat ID: {request.chat_id}, Message: {request.message}")
    async def event_generator():
        import json
        import logging

        response_text = ""
        try:
            # 1. get_streaming_chain í˜¸ì¶œì„ ìˆ˜ì •í•˜ê³ , ë°˜í™˜ ê°’ì˜ ê°œìˆ˜ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬í•©ë‹ˆë‹¤.
            result = get_streaming_chain(request.report_id, request.message)
            is_farewell = False
            if len(result) == 2:
                chain, memory = result
                is_farewell = True
            else:
                chain, memory, _ = result  # turn_countëŠ” ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

            # 2. astreamì„ ì‚¬ìš©í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            stream_input = {}
            if not is_farewell:
                stream_input = {"question": request.message}

            logging.info(f"SSE stream started for chat_id: {request.chat_id}")
            chunk_count = 0
            async for chunk in chain.astream(stream_input):
                chunk_count += 1
                token = ""
                # LangChain ì²´ì¸ì˜ ì¢…ë¥˜ì— ë”°ë¼ ë°˜í™˜ë˜ëŠ” chunkì˜ íƒ€ì…ì´ ë‹¤ë¦…ë‹ˆë‹¤.
                if isinstance(chunk, dict) and "answer" in chunk:
                    token = chunk["answer"]  # ConversationalRetrievalChainì˜ ê²½ìš°
                elif hasattr(chunk, 'content'):
                    token = chunk.content  # LCEL ì²´ì¸ (farewell)ì˜ ê²½ìš°
                
                if token:
                    # í† í°ì„ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ ê°•í™”
                    if len(token) > 10:  # ê¸´ í† í°ì„ ë” ì‘ê²Œ ë¶„í• 
                        words = token.split()
                        for i, word in enumerate(words):
                            if i > 0:
                                word = " " + word
                            logging.info(f"Streaming chunk #{chunk_count}.{i+1}: '{word}'")
                            response_text += word
                            json_data = json.dumps({"token": word})
                            yield json_data
                    else:
                        logging.info(f"Streaming chunk #{chunk_count}: '{token.strip()}' (length: {len(token)})")
                        response_text += token
                        json_data = json.dumps({"token": token})
                        yield json_data
                else:
                    logging.warning(f"Empty token in chunk #{chunk_count}: {chunk}")
            
            logging.info(f"SSE stream finished for chat_id: {request.chat_id} - Total chunks: {chunk_count}, Final response length: {len(response_text)}")
            
            logging.info(f"SSE stream finished for chat_id: {request.chat_id}")
            # ìŠ¤íŠ¸ë¦¬ë° ì„±ê³µ ì‹œ ë¡œê·¸ ì €ì¥
            save_chat_log(db, request.chat_id, RoleEnum.user, request.message)
            if response_text:
                save_chat_log(db, request.chat_id, RoleEnum.ai, response_text)

        except Exception:
            logging.exception("Error during streaming chat")
            error_data = json.dumps({"error": "ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."})
            # [ìˆ˜ì •] ì˜¤ë¥˜ ë°œìƒ ì‹œ 'error' ì´ë²¤íŠ¸ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
            yield {"event": "error", "data": error_data}

        finally:
            # [ìˆ˜ì •] ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œë¥¼ ìœ„í•´ 'end' ì´ë²¤íŠ¸ë¥¼ ì „ì†¡í•©ë‹ˆë‹¤.
            yield {"event": "end", "data": "[DONE]"}

    return EventSourceResponse(event_generator())


@router.post(
    "/create",
    response_model=CreateChatResponse,
    summary="ì±„íŒ…ë°© ìƒì„± ë° AI ì¸ì‚¬",
    description="ì§€ì •í•œ report_idì— ëŒ€í•´ ìƒˆ ì±„íŒ…ë°©ì„ ìƒì„±í•˜ê³ , AIì˜ ì²« ì¸ì‚¬ë§ì„ ë°˜í™˜í•©ë‹ˆë‹¤."
)
def create_chat(
        request: CreateChatRequest,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    if not request.report_id:
        raise HTTPException(status_code=400, detail="report_idëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.")

    report = db.query(Report).filter(Report.report_id == request.report_id).first()
    if not report:
        raise HTTPException(status_code=404, detail="í•´ë‹¹ report_idê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # 1. ì±„íŒ…ë°© ìƒì„±
    chat = Chat(report_id=report.report_id)
    db.add(chat)
    db.commit()
    db.refresh(chat)

    # 2. AIì˜ ì²« ì¸ì‚¬ë§ ìƒì„± ë° ì €ì¥
    initial_greeting = "ì•ˆë…•í•˜ì„¸ìš”. ì§€ê¸ˆë¶€í„° ëŒ€í™”ë¥¼ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ë³´ë‹¤ ì •í™•í•œ ê²€ì‚¬ë¥¼ ìœ„í•´, ë‹¨ë‹µí˜•ë³´ë‹¤ëŠ” ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤."
    save_chat_log(db, chat_id=chat.chat_id, role=RoleEnum.ai, text=initial_greeting)

    # 3. ìƒì„±ëœ chat_idì™€ ì¸ì‚¬ë§ ë°˜í™˜
    return CreateChatResponse(
        chat_id=chat.chat_id,
        message=initial_greeting
    )


@router.get("/logs/{report_id}", response_model=list[ChatLogResponse])
def get_logs_by_report_id(report_id: int, db: Session = Depends(get_db)):
    return get_chat_logs_by_report_id(db, report_id)

@router.get(
    "/logs/{chat_id}",
    response_model=list[ChatLogResponse],
    summary="ì±„íŒ… ë¡œê·¸ ì¡°íšŒ",
    description="íŠ¹ì • chat_idì— ëŒ€í•œ ëª¨ë“  ì±„íŒ… ë¡œê·¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
)
def read_chat_logs(
        chat_id: int,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    return get_chat_logs(db, chat_id)

@router.post(
    "/chats/{chat_id}/evaluate",
    response_model=EvaluateChatResponse,
    summary="ì±„íŒ… í‰ê°€ ë° ê²°ê³¼ ì €ì¥",
    description="chat_idì™€ report_idë¥¼ ê¸°ë°˜ìœ¼ë¡œ AI ë¶„ì„ ê²°ê³¼(ì†Œê²¬, ìœ„í—˜ë„)ë¥¼ í‰ê°€í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."
)
def evaluate_chat_and_save(
        chat_id: int,
        report_id: int,
        db: Session = Depends(get_db),
        current_user: User = Depends(get_current_user)
):
    try:
        chat_result, chat_risk = evaluate_and_save_chat_result(db, chat_id, report_id)
        return EvaluateChatResponse(
            chat_result=chat_result,
            chat_risk=chat_risk,
            message="ì†Œê²¬ ë° ìœ„í—˜ë„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
        )
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

@router.post(
    "/voice",
    response_model=VoiceChatResponse,
    summary="ìŒì„± ì±„íŒ… ì‹œì‘ (STT -> AI -> TTS)",
    description="ìŒì„± íŒŒì¼ì„ ë°›ì•„ ì „ì²´ ë¹„ë™ê¸° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹œì‘í•˜ê³ , ìµœì¢… ì‘ì—… IDë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."
)
async def voice_chat(
        report_id: int = Form(...),
        chat_id: int = Form(...),
        file: UploadFile = File(...),
        current_user: User = Depends(get_current_user)
):
    try:
        audio_content = await file.read()

        # Celery ì‘ì—… ì²´ì¸ ìƒì„±: STT -> AI Chat -> TTS
        # ê° ì‘ì—…ì˜ ê²°ê³¼ê°€ ë‹¤ìŒ ì‘ì—…ì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.
        chain = (
            celery_app.signature("stt_task", args=[audio_content, file.filename]) |
            celery_app.signature("ai_chat_task", args=[report_id, chat_id]) |
            celery_app.signature("tts_task")
        )

        # ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²´ì¸ ì‹¤í–‰
        task_result = chain.apply_async()

        # ì²´ì¸ì˜ ë§ˆì§€ë§‰ ì‘ì—…(TTS)ì˜ IDë¥¼ ë°˜í™˜
        return VoiceChatResponse(task_id=task_result.id)

    except Exception as e:
        # íŒŒì¼ ì²˜ë¦¬ë‚˜ ì‘ì—… ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        raise HTTPException(status_code=500, detail=f"ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@router.get("/task-status/{task_id}")
async def get_task_status(task_id: str):
    """(ìŒì„±ì±„íŒ…) ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    task_result = AsyncResult(task_id, app=celery_app)

    response = {
        "task_id": task_id,
        "status": task_result.status,
        "result": task_result.result if task_result.successful() else None,
    }

    if task_result.failed():
        response["result"] = {
            "error": str(task_result.info),
            "traceback": task_result.traceback,
        }

    return response
